Using TOKENIZER 2: HuggingFace WordPiece (BERT-base-uncased)
Device: cuda | Vocab Size: 30522 | Embed Dim: 240 | PAD IDX: 100
Using AG News Dataset: 1000000 samples
DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'token_ids'],
        num_rows: 120000
    })
    test: Dataset({
        features: ['text', 'label', 'token_ids'],
        num_rows: 7600
    })
})
Randomly Initializing Embeddings/Proj (GloVe not used).
STARTING TRAINING...
-- Epoch 1/1 --
Epoch 1 Complete. | Perplexity/char: 1.3295 | Test BPB: 0.4109 bits/byte | 
Compression Ratio: 0.0514

Compression baselines on TEST corpus (preprocessed text):
  Raw size: 1494718 bytes
  GZIP: 670612 bytes | 3.5892 bits/byte | ratio: 0.4487
  BZ2 : 538176 bytes | 2.8804 bits/byte | ratio: 0.3601
  LZMA: 539984 bytes | 2.8901 bits/byte | ratio: 0.3613


==================================================
TRAINING SUMMARY REPORT (RNN)
  Total Training Time: 318.64 seconds
  Final Avg Training Loss: 6.5278
==================================================

STARTING TESTING...
Testing Complete. Average Test Loss: 6.2244

==================================================
STARTING TOKENIZER EVALUATION (NLS + SUBWORD METRICS)
Tokenizer evaluation on 'A dog is an amazing animal with a heart of a true 
lifemate of men, and with many other qualities'
Word Count: 20 | Char Count: 76
  NLS_w (Tokens per Word): 0.5000
  NLS_c (Tokens per Character): 0.1316
  NLS (Reference T/R): 0.4000 (Efficiency vs. t5)

Subword Metrics:
  Subword Fertility (Ideal: 1.0): 0.5000
  Proportion of Continued Words (Ideal: 0.0): 0.0000
Tokenizer Evaluation Complete.
==================================================


==================================================
RNN MODEL SIZE BREAKDOWN

  Total params      : [22,516,800 (85.89 MB)
  Embeddings (WTE)  : 7,325,280 (27.94 MB)
  Output head (proj): 7,325,280 (27.94 MB)
  Core (LSTMs only) : 7,866,240 (30.01 MB)
==================================================

